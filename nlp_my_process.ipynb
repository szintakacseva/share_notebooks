{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LinkedIn Learning Natural Language Processing tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP Basics: What is Natural Language Processing & the Natural Language Toolkit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "string.punctuation\n",
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting help\n",
    "\n",
    "# help(plt.hist)\n",
    "# help(plt.legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binder upload\n",
    "# 1. goto mybinder.com\n",
    "# 2. add the github repository link, branch, filename, requirements.txt should be filled with dependencies\n",
    "# 3. remove path from read_csv\n",
    "# 4. share the link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "#Download NLTK data\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "# tokenize\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopword]\n",
    "    return text\n",
    "\n",
    "# stem text\n",
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "# lemmatize text\n",
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "# Create feature for % of text that is punctuation\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list nltkfunctions\n",
    "#dir(nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "'''\n",
    "path = 'e:/PycharmProjects/NLP/input'\n",
    "file1 = path +'/spam.csv'\n",
    "file2 = path + '/datasets_483_982_spam.csv'\n",
    "rawData = pd.read_csv(file2,encoding = \"ISO-8859-1\")\n",
    "'''\n",
    "\n",
    "path = os.path.abspath(os.getcwd())\n",
    "datadir = 'data'\n",
    "full_path = os.path.join(path, datadir)\n",
    "spam_file = os.path.join(full_path, \"datasets_483_982_spam.csv\")\n",
    "rawData = pd.read_csv(spam_file, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "fullCorpus = rawData[['v1', 'v2']]\n",
    "fullCorpus.head()\n",
    "fullCorpus.columns = ['label', 'body_text'];\n",
    "fullCorpus.head()\n",
    "data = fullCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data has 5572 rows and 2 columns\n",
      "Out of 5572 rows, 747 are spam, 4825 are ham\n",
      "Number of null in label: 0\n",
      "Number of null in text: 0\n"
     ]
    }
   ],
   "source": [
    "# explore the dataset\n",
    "\n",
    "# What is the shape of the dataset?\n",
    "print(\"Input data has {} rows and {} columns\".format(len(fullCorpus), len(fullCorpus.columns)))\n",
    "\n",
    "# How many spam/ham are there?\n",
    "print(\"Out of {} rows, {} are spam, {} are ham\".format(len(fullCorpus),\n",
    "                                                       len(fullCorpus[fullCorpus['label']=='spam']),\n",
    "                                                       len(fullCorpus[fullCorpus['label']=='ham'])))\n",
    "# How much missing data is there?\n",
    "print(\"Number of null in label: {}\".format(fullCorpus['label'].isnull().sum()))\n",
    "print(\"Number of null in text: {}\".format(fullCorpus['body_text'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'herself', 'been', 'with', 'here', 'very', 'doesn', 'won']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords\n",
    "\n",
    "stopwords.words('english')[0:500:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useing regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_test = 'This is a made up string to test 2 different regex methods'\n",
    "re_test_messy = 'This      is a made up     string to test 2    different regex methods'\n",
    "re_test_messy1 = 'This-is-a-made/up.string*to>>>>test----2\"\"\"\"\"\"different~regex-methods'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'made', 'up', 'string', 'to', 'test', '2', 'different', 'regex', 'methods']\n",
      "['This', '', '', '', '', '', 'is', 'a', 'made', 'up', '', '', '', '', 'string', 'to', 'test', '2', '', '', '', 'different', 'regex', 'methods']\n",
      "['This-is-a-made/up.string*to>>>>test----2\"\"\"\"\"\"different~regex-methods']\n",
      "['This', 'is', 'a', 'made', 'up', 'string', 'to', 'test', '2', 'different', 'regex', 'methods']\n",
      "['', '-', '-', '-', '/', '.', '*', '>>>>', '----', '\"\"\"\"\"\"', '~', '-', '']\n",
      "['', '-', '-', '-', '/', '.', '*', '>>>>', '----', '\"\"\"\"\"\"', '~', '-', '']\n",
      "['This', 'is', 'a', 'made', 'up', 'string', 'to', 'test', '2', 'different', 'regex', 'methods']\n",
      "['', '      ', ' ', ' ', ' ', '     ', ' ', ' ', ' ', '    ', ' ', ' ', '']\n"
     ]
    }
   ],
   "source": [
    "# Splitting a sentence into a list of words\n",
    "\n",
    "print(re.split('\\s', re_test))\n",
    "print(re.split('\\s', re_test_messy))\n",
    "print(re.split('\\s', re_test_messy1))\n",
    "print(re.split('\\s+', re_test_messy))\n",
    "print(re.split('\\w+', re_test_messy1))\n",
    "print(re.split('\\w+', re_test_messy1))\n",
    "print(re.split('\\W+', re_test_messy1))\n",
    "print(re.split('\\S+', re_test_messy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['try', 'to', 'follow', 'guidelines']\n",
      "['I', 'PEP']\n",
      "['PEEP8']\n",
      "I try to follow PEP8 Python Styleguide guidelines\n"
     ]
    }
   ],
   "source": [
    "# Replacing a specific string\n",
    "\n",
    "pep8_test = 'I try to follow PEP8 guidelines'\n",
    "pep7_test = 'I try to follow PEP7 guidelines'\n",
    "peep8_test = 'I try to follow PEEP8 guidelines'\n",
    "\n",
    "print(re.findall('[a-z]+', pep8_test))\n",
    "print(re.findall('[A-Z]+', pep8_test))\n",
    "print(re.findall('[A-Z]+[0-9]+', peep8_test))\n",
    "print(re.sub('[A-Z]+[0-9]+', 'PEP8 Python Styleguide', peep8_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a pipeline to clean text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pre-processing text data\n",
    "\n",
    "Cleaning up the text data is necessary to highlight attributes that you're going to want your machine learning system to pick up on. \n",
    "Cleaning (or pre-processing) the data typically consists of a number of steps:      \n",
    "1.Remove punctuation     \n",
    "2.Tokenization           \n",
    "3.Remove stopwords        \n",
    "4.Lemmatize/Stem           \n",
    "\n",
    "The first three steps are covered in this chapter as they're implemented in pretty much any text cleaning pipeline. Lemmatizing and stemming are covered in the next chapter as they're helpful but not critical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_stemmed</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, n, great, world, la, e, buffet, Cine, got, amore, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, final, tkts, 21st, May, 2005, Text, FA, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "      <td>[Nah, I, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...   \n",
       "1                                                                              Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "3                                                          U dun say so early hor U c already then say   \n",
       "4                                          Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                   body_text_tokenized  \\\n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...   \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "3                                              [u, dun, say, so, early, hor, u, c, already, then, say]   \n",
       "4                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "\n",
       "                                                                                      body_text_nostop  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]   \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "3                                                        [u, dun, say, early, hor, u, c, already, say]   \n",
       "4                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "\n",
       "                                                                                     body_text_stemmed  \\\n",
       "0        [go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]   \n",
       "1                                                                         [ok, lar, joke, wif, u, oni]   \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...   \n",
       "3                                                        [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
       "4                                                   [nah, dont, think, goe, usf, live, around, though]   \n",
       "\n",
       "                                                                                          cleaned_text  \n",
       "0  [Go, jurong, point, crazy, Available, bugis, n, great, world, la, e, buffet, Cine, got, amore, wat]  \n",
       "1                                                                       [Ok, lar, Joking, wif, u, oni]  \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, final, tkts, 21st, May, 2005, Text, FA, 87121, receiv...  \n",
       "3                                                        [U, dun, say, early, hor, U, c, already, say]  \n",
       "4                                              [Nah, I, dont, think, goes, usf, lives, around, though]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuation\n",
    "data['body_text_clean'] = data['body_text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "# tokenize\n",
    "data['body_text_tokenized'] = data['body_text_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "# remove stopwords\n",
    "data['body_text_nostop'] = data['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# clean (remove punctuation, tokenize, remove stopwords) text\n",
    "data['body_text_nostop'] = data['body_text'].apply(lambda x: clean_text(x.lower()))\n",
    "\n",
    "# stemming\n",
    "data['body_text_stemmed'] = data['body_text_nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "# clean text\n",
    "data['cleaned_text'] = data['body_text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "#data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplemental Data Cleaning: Using a Lemmatizer (stemming)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the \n",
    "same stem even if the stem itself is not a valid word in the Language.\n",
    "\n",
    "Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language.\n",
    "In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, \n",
    "or citation form of a set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grow\n",
      "grow\n",
      "grow\n",
      "run\n",
      "run\n",
      "runner\n"
     ]
    }
   ],
   "source": [
    "# examples of stemming\n",
    "print(ps.stem('grows'))\n",
    "print(ps.stem('growing'))\n",
    "print(ps.stem('grow'))\n",
    "\n",
    "print(ps.stem('run'))\n",
    "print(ps.stem('running'))\n",
    "print(ps.stem('runner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_stemmed</th>\n",
       "      <th>body_text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...</td>\n",
       "      <td>FreeMsg Hey there darling its been 3 weeks now and no word back Id like some fun you up for it s...</td>\n",
       "      <td>[freemsg, hey, there, darling, its, been, 3, weeks, now, and, no, word, back, id, like, some, fu...</td>\n",
       "      <td>[freemsg, hey, darling, 3, weeks, word, back, id, like, fun, still, tb, ok, xxx, std, chgs, send...</td>\n",
       "      <td>[freemsg, hey, darl, 3, week, word, back, id, like, fun, still, tb, ok, xxx, std, chg, send, å, ...</td>\n",
       "      <td>[freemsg, hey, darling, 3, week, word, back, id, like, fun, still, tb, ok, xxx, std, chgs, send,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>As per your request Melle Melle Oru Minnaminunginte Nurungu Vettam has been set as your callertu...</td>\n",
       "      <td>[as, per, your, request, melle, melle, oru, minnaminunginte, nurungu, vettam, has, been, set, as...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
       "      <td>[per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To ...</td>\n",
       "      <td>WINNER As a valued network customer you have been selected to receivea å£900 prize reward To cla...</td>\n",
       "      <td>[winner, as, a, valued, network, customer, you, have, been, selected, to, receivea, å, 900, priz...</td>\n",
       "      <td>[winner, valued, network, customer, selected, receivea, å, 900, prize, reward, claim, call, 0906...</td>\n",
       "      <td>[winner, valu, network, custom, select, receivea, å, 900, prize, reward, claim, call, 0906170146...</td>\n",
       "      <td>[winner, valued, network, customer, selected, receivea, å, 900, prize, reward, claim, call, 0906...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with came...</td>\n",
       "      <td>Had your mobile 11 months or more U R entitled to Update to the latest colour mobiles with camer...</td>\n",
       "      <td>[had, your, mobile, 11, months, or, more, u, r, entitled, to, update, to, the, latest, colour, m...</td>\n",
       "      <td>[mobile, 11, months, u, r, entitled, update, latest, colour, mobiles, camera, free, call, mobile...</td>\n",
       "      <td>[mobil, 11, month, u, r, entitl, updat, latest, colour, mobil, camera, free, call, mobil, updat,...</td>\n",
       "      <td>[mobile, 11, month, u, r, entitled, update, latest, colour, mobile, camera, free, call, mobile, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "5  spam   \n",
       "6   ham   \n",
       "7   ham   \n",
       "8  spam   \n",
       "9  spam   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "5  FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...   \n",
       "6                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "7  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "8  WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To ...   \n",
       "9  Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with came...   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...   \n",
       "1                                                                              Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "3                                                          U dun say so early hor U c already then say   \n",
       "4                                          Nah I dont think he goes to usf he lives around here though   \n",
       "5  FreeMsg Hey there darling its been 3 weeks now and no word back Id like some fun you up for it s...   \n",
       "6                          Even my brother is not like to speak with me They treat me like aids patent   \n",
       "7  As per your request Melle Melle Oru Minnaminunginte Nurungu Vettam has been set as your callertu...   \n",
       "8  WINNER As a valued network customer you have been selected to receivea å£900 prize reward To cla...   \n",
       "9  Had your mobile 11 months or more U R entitled to Update to the latest colour mobiles with camer...   \n",
       "\n",
       "                                                                                   body_text_tokenized  \\\n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...   \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "3                                              [u, dun, say, so, early, hor, u, c, already, then, say]   \n",
       "4                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "5  [freemsg, hey, there, darling, its, been, 3, weeks, now, and, no, word, back, id, like, some, fu...   \n",
       "6         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n",
       "7  [as, per, your, request, melle, melle, oru, minnaminunginte, nurungu, vettam, has, been, set, as...   \n",
       "8  [winner, as, a, valued, network, customer, you, have, been, selected, to, receivea, å, 900, priz...   \n",
       "9  [had, your, mobile, 11, months, or, more, u, r, entitled, to, update, to, the, latest, colour, m...   \n",
       "\n",
       "                                                                                      body_text_nostop  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]   \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "3                                                        [u, dun, say, early, hor, u, c, already, say]   \n",
       "4                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "5  [freemsg, hey, darling, 3, weeks, word, back, id, like, fun, still, tb, ok, xxx, std, chgs, send...   \n",
       "6                                              [even, brother, like, speak, treat, like, aids, patent]   \n",
       "7  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...   \n",
       "8  [winner, valued, network, customer, selected, receivea, å, 900, prize, reward, claim, call, 0906...   \n",
       "9  [mobile, 11, months, u, r, entitled, update, latest, colour, mobiles, camera, free, call, mobile...   \n",
       "\n",
       "                                                                                     body_text_stemmed  \\\n",
       "0        [go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]   \n",
       "1                                                                         [ok, lar, joke, wif, u, oni]   \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...   \n",
       "3                                                        [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
       "4                                                   [nah, dont, think, goe, usf, live, around, though]   \n",
       "5  [freemsg, hey, darl, 3, week, word, back, id, like, fun, still, tb, ok, xxx, std, chg, send, å, ...   \n",
       "6                                               [even, brother, like, speak, treat, like, aid, patent]   \n",
       "7  [per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...   \n",
       "8  [winner, valu, network, custom, select, receivea, å, 900, prize, reward, claim, call, 0906170146...   \n",
       "9  [mobil, 11, month, u, r, entitl, updat, latest, colour, mobil, camera, free, call, mobil, updat,...   \n",
       "\n",
       "                                                                                  body_text_lemmatized  \n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]  \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "3                                                        [u, dun, say, early, hor, u, c, already, say]  \n",
       "4                                                    [nah, dont, think, go, usf, life, around, though]  \n",
       "5  [freemsg, hey, darling, 3, week, word, back, id, like, fun, still, tb, ok, xxx, std, chgs, send,...  \n",
       "6                                               [even, brother, like, speak, treat, like, aid, patent]  \n",
       "7  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...  \n",
       "8  [winner, valued, network, customer, selected, receivea, å, 900, prize, reward, claim, call, 0906...  \n",
       "9  [mobile, 11, month, u, r, entitled, update, latest, colour, mobile, camera, free, call, mobile, ...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['body_text_lemmatized'] = data['body_text_nostop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Creates a document-term matrix where the entry of each cell will be a count of the number of times that word occurred in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "['CSV', 'DataFrame', 'Database', 'Excel', 'In', 'Pandas', 'SQL', 'created', 'csv', 'datasets', 'dictionary', 'etc', 'existing', 'file', 'list', 'lists', 'loading', 'real', 'storage', 'world']\n",
      "[[3 3 1 2 1 2 1 1 1 1 2 1 1 2 1 1 2 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "\n",
    "mytext = '''In the real world, a Pandas DataFrame will be created DataFrame loading the datasets from existing storage,\n",
    "storage can be SQL Database, CSV file, and Excel file. Pandas DataFrame can be loading from the lists, dictionary, \n",
    "and from a list of dictionary etc CSV Excel CSV csv''';\n",
    "\n",
    "mydata = {'body':  [mytext]}\n",
    "\n",
    "df = pd.DataFrame(mydata)\n",
    "\n",
    "X_counts = count_vect.fit_transform(data['body_text'])\n",
    "X_counts = count_vect.fit_transform(df['body'])\n",
    "\n",
    "print(X_counts.shape)\n",
    "print(count_vect.get_feature_names())\n",
    "print(X_counts.toarray())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Apply CountVectorizer to smaller sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Go': 51, 'jurong': 165, 'point': 192, 'crazy': 134, 'Available': 28, 'bugis': 121, 'n': 181, 'great': 158, 'world': 249, 'la': 168, 'e': 141, 'buffet': 120, 'Cine': 35, 'got': 155, 'amore': 111, 'wat': 236, 'Ok': 75, 'lar': 169, 'Joking': 62, 'wif': 242, 'u': 229, 'oni': 189, 'Free': 48, 'entry': 146, '2': 11, 'wkly': 244, 'comp': 132, 'win': 243, 'FA': 45, 'Cup': 39, 'final': 148, 'tkts': 224, '21st': 14, 'May': 66, '2005': 13, 'Text': 88, '87121': 22, 'receive': 199, 'questionstd': 196, 'txt': 228, 'rateTCs': 197, 'apply': 113, '08452810075over18s': 1, 'U': 96, 'dun': 140, 'say': 205, 'early': 142, 'hor': 161, 'c': 122, 'already': 110, 'Nah': 70, 'I': 57, 'dont': 139, 'think': 221, 'goes': 153, 'usf': 232, 'lives': 173, 'around': 114, 'though': 222, 'FreeMsg': 49, 'Hey': 56, 'darling': 138, '3': 15, 'weeks': 240, 'word': 247, 'back': 116, 'Id': 58, 'like': 171, 'fun': 151, 'still': 214, 'Tb': 87, 'ok': 188, 'XxX': 106, 'std': 213, 'chgs': 127, 'send': 208, 'å': 252, '150': 8, 'rcv': 198, 'Even': 44, 'brother': 119, 'speak': 211, 'They': 91, 'treat': 227, 'aids': 109, 'patent': 190, 'As': 27, 'per': 191, 'request': 202, 'Melle': 67, 'Oru': 76, 'Minnaminunginte': 68, 'Nurungu': 72, 'Vettam': 100, 'set': 209, 'callertune': 124, 'Callers': 33, 'Press': 79, '9': 24, 'copy': 133, 'friends': 149, 'Callertune': 34, 'WINNER': 103, 'valued': 234, 'network': 185, 'customer': 137, 'selected': 207, 'receivea': 200, '900': 25, 'prize': 194, 'reward': 203, 'To': 92, 'claim': 128, 'call': 123, '09061701461': 2, 'Claim': 36, 'code': 130, 'KL341': 63, 'Valid': 99, '12': 7, 'hours': 162, 'Had': 54, 'mobile': 178, '11': 6, 'months': 180, 'R': 81, 'entitled': 145, 'Update': 98, 'latest': 170, 'colour': 131, 'mobiles': 179, 'camera': 125, 'Call': 32, 'The': 90, 'Mobile': 69, 'Co': 37, 'FREE': 46, '08002986030': 0, 'Im': 59, 'gonna': 154, 'home': 160, 'soon': 210, 'want': 235, 'talk': 217, 'stuff': 215, 'anymore': 112, 'tonight': 226, 'k': 166, 'Ive': 60, 'cried': 136, 'enough': 144, 'today': 225, 'SIX': 84, 'chances': 126, 'CASH': 29, 'From': 50, '100': 4, '20000': 12, 'pounds': 193, 'CSH11': 31, '87575': 23, 'Cost': 38, '150pday': 9, '6days': 19, '16': 10, 'TsandCs': 94, 'Reply': 82, 'HL': 53, '4': 16, 'info': 164, 'URGENT': 97, 'You': 108, '1': 3, 'week': 239, 'membership': 175, '100000': 5, 'Prize': 80, 'Jackpot': 61, 'Txt': 95, 'CLAIM': 30, 'No': 71, '81010': 20, 'TC': 86, 'wwwdbuknet': 250, 'LCCLTD': 64, 'POBOX': 77, '4403LDNW1A7RW18': 17, 'searching': 206, 'right': 204, 'words': 248, 'thank': 219, 'breather': 118, 'promise': 195, 'wont': 246, 'take': 216, 'help': 159, 'granted': 157, 'fulfil': 150, 'wonderful': 245, 'blessing': 117, 'times': 223, 'HAVE': 52, 'A': 26, 'DATE': 40, 'ON': 73, 'SUNDAY': 85, 'WITH': 104, 'WILL': 102, 'XXXMobileMovieClub': 105, 'use': 231, 'credit': 135, 'click': 129, 'WAP': 101, 'link': 172, 'next': 187, 'message': 176, 'httpwap': 163, 'xxxmobilemovieclubcomnQJKGIGHJJGCBL': 251, 'Oh': 74, 'kim': 167, 'watching': 237, 'Eh': 42, 'remember': 201, 'spell': 212, 'name': 182, 'Yes': 107, 'He': 55, 'v': 233, 'naughty': 184, 'make': 174, 'wet': 241, 'Fine': 47, 'thatåÕs': 220, 'way': 238, 'feel': 147, 'ThatåÕs': 89, 'gota': 156, 'b': 115, 'England': 43, 'Macedonia': 65, 'miss': 177, 'goalsteam': 152, 'news': 186, 'ur': 230, 'national': 183, 'team': 218, '87077': 21, 'eg': 143, 'ENGLAND': 41, 'TryWALES': 93, 'SCOTLAND': 83, '4txtÌ¼120': 18, 'POBOXox36504W45WQ': 78}\n",
      "(20, 253)\n",
      "['08002986030', '08452810075over18s', '09061701461', '1', '100', '100000', '11', '12', '150', '150pday', '16', '2', '20000', '2005', '21st', '3', '4', '4403LDNW1A7RW18', '4txtÌ¼120', '6days', '81010', '87077', '87121', '87575', '9', '900', 'A', 'As', 'Available', 'CASH', 'CLAIM', 'CSH11', 'Call', 'Callers', 'Callertune', 'Cine', 'Claim', 'Co', 'Cost', 'Cup', 'DATE', 'ENGLAND', 'Eh', 'England', 'Even', 'FA', 'FREE', 'Fine', 'Free', 'FreeMsg', 'From', 'Go', 'HAVE', 'HL', 'Had', 'He', 'Hey', 'I', 'Id', 'Im', 'Ive', 'Jackpot', 'Joking', 'KL341', 'LCCLTD', 'Macedonia', 'May', 'Melle', 'Minnaminunginte', 'Mobile', 'Nah', 'No', 'Nurungu', 'ON', 'Oh', 'Ok', 'Oru', 'POBOX', 'POBOXox36504W45WQ', 'Press', 'Prize', 'R', 'Reply', 'SCOTLAND', 'SIX', 'SUNDAY', 'TC', 'Tb', 'Text', 'ThatåÕs', 'The', 'They', 'To', 'TryWALES', 'TsandCs', 'Txt', 'U', 'URGENT', 'Update', 'Valid', 'Vettam', 'WAP', 'WILL', 'WINNER', 'WITH', 'XXXMobileMovieClub', 'XxX', 'Yes', 'You', 'aids', 'already', 'amore', 'anymore', 'apply', 'around', 'b', 'back', 'blessing', 'breather', 'brother', 'buffet', 'bugis', 'c', 'call', 'callertune', 'camera', 'chances', 'chgs', 'claim', 'click', 'code', 'colour', 'comp', 'copy', 'crazy', 'credit', 'cried', 'customer', 'darling', 'dont', 'dun', 'e', 'early', 'eg', 'enough', 'entitled', 'entry', 'feel', 'final', 'friends', 'fulfil', 'fun', 'goalsteam', 'goes', 'gonna', 'got', 'gota', 'granted', 'great', 'help', 'home', 'hor', 'hours', 'httpwap', 'info', 'jurong', 'k', 'kim', 'la', 'lar', 'latest', 'like', 'link', 'lives', 'make', 'membership', 'message', 'miss', 'mobile', 'mobiles', 'months', 'n', 'name', 'national', 'naughty', 'network', 'news', 'next', 'ok', 'oni', 'patent', 'per', 'point', 'pounds', 'prize', 'promise', 'questionstd', 'rateTCs', 'rcv', 'receive', 'receivea', 'remember', 'request', 'reward', 'right', 'say', 'searching', 'selected', 'send', 'set', 'soon', 'speak', 'spell', 'std', 'still', 'stuff', 'take', 'talk', 'team', 'thank', 'thatåÕs', 'think', 'though', 'times', 'tkts', 'today', 'tonight', 'treat', 'txt', 'u', 'ur', 'use', 'usf', 'v', 'valued', 'want', 'wat', 'watching', 'way', 'week', 'weeks', 'wet', 'wif', 'win', 'wkly', 'wonderful', 'wont', 'word', 'words', 'world', 'wwwdbuknet', 'xxxmobilemovieclubcomnQJKGIGHJJGCBL', 'å']\n"
     ]
    }
   ],
   "source": [
    "# apply CountVectorizer to smaller sample\n",
    "data_sample = data[0:20]\n",
    "\n",
    "count_vect_sample = CountVectorizer(analyzer=clean_text)\n",
    "encoded_counts_sample = count_vect_sample.fit(data_sample['body_text'])\n",
    "print(count_vect_sample.vocabulary_)\n",
    "\n",
    "X_counts_sample = count_vect_sample.fit_transform(data_sample['body_text'])\n",
    "print(X_counts_sample.shape)\n",
    "print(count_vect_sample.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vectorizers output sparse matrices.\n",
    "\n",
    "Sparse Matrix: A matrix in which most entries are 0. In the interest of efficient storage, a sparse matrix will be stored by only storing the locations of the non-zero elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse matrix\n",
    "\n",
    "X_counts_sample\n",
    "X_counts_df = pd.DataFrame(X_counts_sample.toarray())\n",
    "X_counts_df\n",
    "\n",
    "X_counts_df.columns = count_vect_sample.get_feature_names()\n",
    "X_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[3 4 1 1 1 1 1 2]]\n",
      "['brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the']\n"
     ]
    }
   ],
   "source": [
    "# Simple CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the brown, brown,lazy dog dog dog dog.\"]\n",
    "text1 = [\"The brown, brown,lazy dog dog dog dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vectorizing Raw Data: N-Grams\n",
    "\n",
    "N-Grams\n",
    "Creates a document-term matrix where counts still occupy the cell but instead of the columns representing single terms, they represent all combinations of adjacent words of length n in your text.\n",
    "\n",
    "Example: \"NLP is an interesting topic\"\n",
    "\n",
    "n Name Tokens\n",
    "\n",
    "2 bigram [\"nlp is\", \"is an\", \"an interesting\", \"interesting topic\"] \n",
    "3 trigram [\"nlp is an\", \"is an interesting\", \"an interesting topic\"] \n",
    "4 four-gram [\"nlp is an interesting\", \"is an interesting topic\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "text = ['''I have posted my sample train data as well as test data along with my code. I'm trying to use Naive \n",
    "           Bayes algorithm to train the model.''']\n",
    "\n",
    "# initialise n-gram vectoriser \n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "X_counts = ngram_vectorizer.fit_transform(data['body_text_clean'][0:3])\n",
    "\n",
    "#print(ngram_vectorizer.vocabulary_)\n",
    "#print(X_counts.shape)\n",
    "#print(ngram_vectorizer.get_feature_names())\n",
    "#print(X_counts.toarray())\n",
    "\n",
    "# create a dataframe from a word matrix\n",
    "df = pd.DataFrame(data=X_counts.toarray(),\n",
    "                      columns=ngram_vectorizer.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vectorizing Raw Data: TF-IDF\n",
    "    \n",
    "Creates a document-term matrix where the columns represent single unique terms (unigrams) but the cell represents a \n",
    "weighting meant to represent how important a word is to a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'][0:3])\n",
    "\n",
    "\n",
    "# print(X_tfidf.shape)\n",
    "# print(tfidf_vect.get_feature_names())\n",
    "\n",
    "df = pd.DataFrame(data=X_tfidf.toarray(),\n",
    "                      columns=tfidf_vect.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature for text message length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_stemmed</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>body_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, n, great, world, la, e, buffet, Cine, got, amore, wat]</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, final, tkts, 21st, May, 2005, Text, FA, 87121, receiv...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "      <td>[Nah, I, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...   \n",
       "1                                                                              Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "3                                                          U dun say so early hor U c already then say   \n",
       "4                                          Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                   body_text_tokenized  \\\n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...   \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "3                                              [u, dun, say, so, early, hor, u, c, already, then, say]   \n",
       "4                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "\n",
       "                                                                                      body_text_nostop  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]   \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "3                                                        [u, dun, say, early, hor, u, c, already, say]   \n",
       "4                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "\n",
       "                                                                                     body_text_stemmed  \\\n",
       "0        [go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]   \n",
       "1                                                                         [ok, lar, joke, wif, u, oni]   \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...   \n",
       "3                                                        [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
       "4                                                   [nah, dont, think, goe, usf, live, around, though]   \n",
       "\n",
       "                                                                                          cleaned_text  \\\n",
       "0  [Go, jurong, point, crazy, Available, bugis, n, great, world, la, e, buffet, Cine, got, amore, wat]   \n",
       "1                                                                       [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, final, tkts, 21st, May, 2005, Text, FA, 87121, receiv...   \n",
       "3                                                        [U, dun, say, early, hor, U, c, already, say]   \n",
       "4                                              [Nah, I, dont, think, goes, usf, lives, around, though]   \n",
       "\n",
       "   body_len  \n",
       "0        92  \n",
       "1        24  \n",
       "2       128  \n",
       "3        39  \n",
       "4        49  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature for text message length\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature for % of text that is punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_stemmed</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, n, great, world, la, e, buffet, Cine, got, amore, wat]</td>\n",
       "      <td>92</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>24</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, final, tkts, 21st, May, 2005, Text, FA, 87121, receiv...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "      <td>39</td>\n",
       "      <td>15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "      <td>[Nah, I, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...   \n",
       "1                                                                              Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "3                                                          U dun say so early hor U c already then say   \n",
       "4                                          Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                   body_text_tokenized  \\\n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...   \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "3                                              [u, dun, say, so, early, hor, u, c, already, then, say]   \n",
       "4                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "\n",
       "                                                                                      body_text_nostop  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]   \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "3                                                        [u, dun, say, early, hor, u, c, already, say]   \n",
       "4                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "\n",
       "                                                                                     body_text_stemmed  \\\n",
       "0        [go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]   \n",
       "1                                                                         [ok, lar, joke, wif, u, oni]   \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...   \n",
       "3                                                        [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
       "4                                                   [nah, dont, think, goe, usf, live, around, though]   \n",
       "\n",
       "                                                                                          cleaned_text  \\\n",
       "0  [Go, jurong, point, crazy, Available, bugis, n, great, world, la, e, buffet, Cine, got, amore, wat]   \n",
       "1                                                                       [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, final, tkts, 21st, May, 2005, Text, FA, 87121, receiv...   \n",
       "3                                                        [U, dun, say, early, hor, U, c, already, say]   \n",
       "4                                              [Nah, I, dont, think, goes, usf, lives, around, though]   \n",
       "\n",
       "   body_len  punct%  \n",
       "0        92     9.8  \n",
       "1        24    25.0  \n",
       "2       128     4.7  \n",
       "3        39    15.4  \n",
       "4        49     4.1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature for % of text that is punctuation\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate created features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate created features\n",
    "\n",
    "bins = np.linspace(0, 200, 40)\n",
    "\n",
    "pyplot.hist(data[data['label']=='spam']['body_len'], bins, alpha=0.5, density=True, label='spam')\n",
    "pyplot.hist(data[data['label']=='ham']['body_len'], bins, alpha=0.5, density=True, label='ham')\n",
    "pyplot.legend(loc='upper left')\n",
    "pyplot.show()\n",
    "\n",
    "bins = np.linspace(0, 50, 40)\n",
    "\n",
    "pyplot.hist(data[data['label']=='spam']['punct%'], bins, alpha=0.5, density=True, label='spam')\n",
    "pyplot.hist(data[data['label']=='ham']['punct%'], bins, alpha=0.5, density=True, label='ham')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate created features\n",
    "\n",
    "bins = np.linspace(0, 200, 40)\n",
    "\n",
    "pyplot.hist(data['body_len'], bins)\n",
    "pyplot.title(\"Body Length Distribution\")\n",
    "pyplot.show()\n",
    "\n",
    "bins = np.linspace(0, 50, 40)\n",
    "\n",
    "pyplot.hist(data['punct%'], bins)\n",
    "pyplot.title(\"Punctuation % Distribution\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power transformations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "POWER TRANSFORMATIONS\n",
    "\n",
    "Power transforms refer to a class of techniques that use a power function (like a logarithm or exponent) to make the probability distribution of a variable Gaussian or more-Gaussian like.\n",
    "A power transform will make the probability distribution of a variable more Gaussian.\n",
    "This is often described as removing a skew in the distribution, although more generally is described as  stabilizing the variance of the distribution.\n",
    "We can apply a power transform directly by calculating the log or square root of the variable, although this may or may not be the best power transform for a given variable.\n",
    "\n",
    "Instead, we can use a generalized version of the transform that finds a parameter (lambda) that best transforms a variable to a Gaussian probability distribution.\n",
    "\n",
    "There are two popular approaches for such automatic power transforms; they are:\n",
    "    Box-Cox Transform\n",
    "    Yeo-Johnson Transform\n",
    "\n",
    "The transformed training dataset can then be fed to a machine learning model to learn a predictive modeling task.\n",
    "A hyperparameter, often referred to as lambda  is used to control the nature of the transform.\n",
    "\n",
    "Below are some common values for lambda\n",
    "\n",
    "    lambda = -1. is a reciprocal transform.\n",
    "    lambda = -0.5 is a reciprocal square root transform.\n",
    "    lambda = 0.0 is a log transform.\n",
    "    lambda = 0.5 is a square root transform.\n",
    "    lambda = 1.0 is no transform.\n",
    "\n",
    "The optimal value for this hyperparameter used in the transform for each variable can be stored and reused to transform new data in the future in an identical manner, such as a test dataset or new data in the future.\n",
    "These power transforms are available in the scikit-learn Python machine learning library via the PowerTransformer class.\n",
    "\n",
    "The class takes an argument named “method” that can be set to ‘yeo-johnson‘ or ‘box-cox‘ for the preferred method. It will also standardize the data automatically after the transform, meaning each variable will have a zero mean and unit variance. This can be turned off by setting the “standardize” argument to False.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Box-Cox Transform\n",
    "\n",
    "It is a power transform that assumes the values of the input variable to which it is applied are strictly positive. That means 0 and negative values are not supported.\n",
    "We can apply the Box-Cox transform using the PowerTransformer class and setting the “method” argument to “box-cox“. Once defined, we can call the fit_transform() function and pass it to our dataset to create a Box-Cox transformed version of our dataset. \n",
    "One way to solve this problem is to use a MixMaxScaler transform first to scale the data to positive values, then apply the transform."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Yeo-Johnson Transform\n",
    "\n",
    "Unlike the Box-Cox transform, it does not require the values for each input variable to be strictly positive. It supports zero values and negative values. This means we can apply it to our dataset without scaling it first.\n",
    "\n",
    "We can apply the transform by defining a PowerTransform object and setting the “method” argument to “yeo-johnson” (the default).\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# General procedure\n",
    "\n",
    "Process\n",
    "1.Determine what range of exponents to test\n",
    "2.Apply each transformation to each value of your chosen feature\n",
    "3.Use some criteria to determine which of the transformations yield the best distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the punctuation % feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional on Power transformations\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "data = pt.fit_transform(data)\n",
    "# convert the array back to a dataframe\n",
    "\n",
    "# SMOTE for imbalanced classification with python\n",
    "# How to Selectively Scale Numerical Input Variables for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Machine Learning Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8050</th>\n",
       "      <th>8051</th>\n",
       "      <th>8052</th>\n",
       "      <th>8053</th>\n",
       "      <th>8054</th>\n",
       "      <th>8055</th>\n",
       "      <th>8056</th>\n",
       "      <th>8057</th>\n",
       "      <th>8058</th>\n",
       "      <th>8059</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8062 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%    0    1    2    3    4    5    6    7  ...  8050  8051  \\\n",
       "0        92     9.8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1        24    25.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2       128     4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3        39    15.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4        49     4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "   8052  8053  8054  8055  8056  8057  8058  8059  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8062 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# apply tfidf tokenization\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "\n",
    "# create new features\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "X_features = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a basic Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97399103, 0.97578475, 0.97935368, 0.97037702, 0.97396768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a basic Random Forest Classifier\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "k_fold = KFold(n_splits=5)\n",
    "cross_val_score(rf, X_features, data['label'], cv=k_fold, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 / Recall: 0.616 / Accuracy: 0.945\n"
     ]
    }
   ],
   "source": [
    "# Random Forest on a holdout test set\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1)\n",
    "rf_model = rf.fit(X_train, y_train)\n",
    "\n",
    "sorted(zip(rf_model.feature_importances_, X_train.columns), reverse=True)[0:10]\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3),\n",
    "                                                        round(recall, 3),\n",
    "                                                        round((y_pred==y_test).sum() / len(y_pred),3)))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 10 / Depth: 10 ---- Precision: 1.0 / Recall: 0.371 / Accuracy: 0.91\n",
      "Est: 10 / Depth: 20 ---- Precision: 1.0 / Recall: 0.642 / Accuracy: 0.949\n",
      "Est: 10 / Depth: 30 ---- Precision: 1.0 / Recall: 0.686 / Accuracy: 0.955\n",
      "Est: 10 / Depth: None ---- Precision: 0.984 / Recall: 0.792 / Accuracy: 0.969\n",
      "Est: 50 / Depth: 10 ---- Precision: 1.0 / Recall: 0.233 / Accuracy: 0.891\n",
      "Est: 50 / Depth: 20 ---- Precision: 1.0 / Recall: 0.547 / Accuracy: 0.935\n",
      "Est: 50 / Depth: 30 ---- Precision: 1.0 / Recall: 0.704 / Accuracy: 0.958\n",
      "Est: 50 / Depth: None ---- Precision: 1.0 / Recall: 0.824 / Accuracy: 0.975\n",
      "Est: 100 / Depth: 10 ---- Precision: 1.0 / Recall: 0.302 / Accuracy: 0.9\n",
      "Est: 100 / Depth: 20 ---- Precision: 1.0 / Recall: 0.623 / Accuracy: 0.946\n",
      "Est: 100 / Depth: 30 ---- Precision: 1.0 / Recall: 0.717 / Accuracy: 0.96\n",
      "Est: 100 / Depth: None ---- Precision: 1.0 / Recall: 0.818 / Accuracy: 0.974\n"
     ]
    }
   ],
   "source": [
    "# Random Forest model check :: Build our own Grid-search\n",
    "def train_RF(n_est, depth):\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1)\n",
    "    rf_model = rf.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print('Est: {} / Depth: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        n_est, depth, round(precision, 3), round(recall, 3),\n",
    "        round((y_pred==y_test).sum() / len(y_pred), 3)))\n",
    "\n",
    "for n_est in [10, 50, 100]:\n",
    "    for depth in [10, 20, 30, None]:\n",
    "        train_RF(n_est, depth)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Random Forest model check with grid-search\n",
    "\n",
    "Grid-search, Exhaustively search all parameter combinations in a given grid to determine the best model.\n",
    "Cross-validation: Divide a dataset into k subsets and repeat the holdout method k times where a different subset is used as the holdout set in each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>92.269819</td>\n",
       "      <td>0.907065</td>\n",
       "      <td>0.920473</td>\n",
       "      <td>0.151004</td>\n",
       "      <td>90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 300}</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>0.969479</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>0.975412</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>46.646538</td>\n",
       "      <td>1.492864</td>\n",
       "      <td>0.719282</td>\n",
       "      <td>0.246484</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 150}</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>0.971275</td>\n",
       "      <td>0.973070</td>\n",
       "      <td>0.974694</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>52.915152</td>\n",
       "      <td>2.183563</td>\n",
       "      <td>1.010898</td>\n",
       "      <td>0.349232</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 150}</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>0.973991</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>0.970377</td>\n",
       "      <td>0.973968</td>\n",
       "      <td>0.974336</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>94.001685</td>\n",
       "      <td>1.189360</td>\n",
       "      <td>0.978436</td>\n",
       "      <td>0.221797</td>\n",
       "      <td>None</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 300}</td>\n",
       "      <td>0.974888</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>0.969479</td>\n",
       "      <td>0.974865</td>\n",
       "      <td>0.974336</td>\n",
       "      <td>0.002517</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.514286</td>\n",
       "      <td>0.539923</td>\n",
       "      <td>0.367794</td>\n",
       "      <td>0.065158</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 10}</td>\n",
       "      <td>0.974888</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.968582</td>\n",
       "      <td>0.973968</td>\n",
       "      <td>0.972172</td>\n",
       "      <td>0.973617</td>\n",
       "      <td>0.003249</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "8       92.269819      0.907065         0.920473        0.151004   \n",
       "7       46.646538      1.492864         0.719282        0.246484   \n",
       "10      52.915152      2.183563         1.010898        0.349232   \n",
       "11      94.001685      1.189360         0.978436        0.221797   \n",
       "6        4.514286      0.539923         0.367794        0.065158   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "8               90                300   \n",
       "7               90                150   \n",
       "10            None                150   \n",
       "11            None                300   \n",
       "6               90                 10   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "8     {'max_depth': 90, 'n_estimators': 300}           0.976682   \n",
       "7     {'max_depth': 90, 'n_estimators': 150}           0.975785   \n",
       "10  {'max_depth': None, 'n_estimators': 150}           0.976682   \n",
       "11  {'max_depth': None, 'n_estimators': 300}           0.974888   \n",
       "6      {'max_depth': 90, 'n_estimators': 10}           0.974888   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "8            0.977578           0.976661           0.969479   \n",
       "7            0.976682           0.976661           0.971275   \n",
       "10           0.973991           0.976661           0.970377   \n",
       "11           0.975785           0.976661           0.969479   \n",
       "6            0.978475           0.968582           0.973968   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "8            0.976661         0.975412        0.002987                1  \n",
       "7            0.973070         0.974694        0.002160                2  \n",
       "10           0.973968         0.974336        0.002317                3  \n",
       "11           0.974865         0.974336        0.002517                3  \n",
       "6            0.972172         0.973617        0.003249                5  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest model check with grid-search\n",
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [10, 150, 300],\n",
    "        'max_depth': [30, 60, 90, None]}\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs_fit = gs.fit(X_features, data['label'])\n",
    "\n",
    "\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Gradient Boosting model (with grid-search)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Building Gradient Boosting model with grid-search\n",
    "Grid-search: Exhaustively search all parameter combinations in a given grid to determine the best model.\n",
    "Explore GradientBoostingClassifier Attributes & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier:: Build our own Grid-search\n",
    "\n",
    "def train_GB(est, max_depth, lr):\n",
    "    gb = GradientBoostingClassifier(n_estimators=est, max_depth=max_depth, learning_rate=lr)\n",
    "    gb_model = gb.fit(X_train, y_train)\n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print('Est: {} / Depth: {} / LR: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        est, max_depth, lr, round(precision, 3), round(recall, 3), \n",
    "        round((y_pred==y_test).sum()/len(y_pred), 3)))\n",
    "    \n",
    "for n_est in [50, 100, 150]:\n",
    "    for max_depth in [3, 7, 11, 15]:\n",
    "        for lr in [0.01, 0.1, 1]:\n",
    "            train_GB(n_est, max_depth, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>448.244170</td>\n",
       "      <td>14.546160</td>\n",
       "      <td>0.621032</td>\n",
       "      <td>0.220888</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}</td>\n",
       "      <td>0.970404</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.973070</td>\n",
       "      <td>0.969479</td>\n",
       "      <td>0.967684</td>\n",
       "      <td>0.971644</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>544.594816</td>\n",
       "      <td>78.519455</td>\n",
       "      <td>0.344001</td>\n",
       "      <td>0.158821</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 100}</td>\n",
       "      <td>0.966816</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.972172</td>\n",
       "      <td>0.964991</td>\n",
       "      <td>0.967684</td>\n",
       "      <td>0.970029</td>\n",
       "      <td>0.004841</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>76.572180</td>\n",
       "      <td>5.459764</td>\n",
       "      <td>0.363821</td>\n",
       "      <td>0.059442</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 10}</td>\n",
       "      <td>0.965022</td>\n",
       "      <td>0.968610</td>\n",
       "      <td>0.963196</td>\n",
       "      <td>0.958707</td>\n",
       "      <td>0.958707</td>\n",
       "      <td>0.962850</td>\n",
       "      <td>0.003804</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>43.590549</td>\n",
       "      <td>0.486445</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10}</td>\n",
       "      <td>0.956054</td>\n",
       "      <td>0.950673</td>\n",
       "      <td>0.956014</td>\n",
       "      <td>0.954219</td>\n",
       "      <td>0.958707</td>\n",
       "      <td>0.955133</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "1     448.244170     14.546160         0.621032        0.220888   \n",
       "3     544.594816     78.519455         0.344001        0.158821   \n",
       "2      76.572180      5.459764         0.363821        0.059442   \n",
       "0      43.590549      0.486445         0.430625        0.009479   \n",
       "\n",
       "  param_learning_rate param_max_depth param_n_estimators  \\\n",
       "1                 0.1               7                100   \n",
       "3                 0.1              11                100   \n",
       "2                 0.1              11                 10   \n",
       "0                 0.1               7                 10   \n",
       "\n",
       "                                                         params  \\\n",
       "1   {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}   \n",
       "3  {'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 100}   \n",
       "2   {'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 10}   \n",
       "0    {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "1           0.970404           0.977578           0.973070           0.969479   \n",
       "3           0.966816           0.978475           0.972172           0.964991   \n",
       "2           0.965022           0.968610           0.963196           0.958707   \n",
       "0           0.956054           0.950673           0.956014           0.954219   \n",
       "\n",
       "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "1           0.967684         0.971644        0.003440                1  \n",
       "3           0.967684         0.970029        0.004841                2  \n",
       "2           0.958707         0.962850        0.003804                3  \n",
       "0           0.958707         0.955133        0.002651                4  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting Classifier :: Exploring parameter settings using GridSearchCV\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "param = {\n",
    "    'n_estimators': [10, 100], \n",
    "    'max_depth': [7, 11],\n",
    "    'learning_rate': [0.1]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(gb, param, cv=5, n_jobs=-1)\n",
    "cv_fit = clf.fit(X_features, data['label'])\n",
    "pd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final evaluation of models :: Random Forest or Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 4.28 / Predict time: 0.18 ---- Precision: 1.0 / Recall: 0.848 / Accuracy: 0.978\n"
     ]
    }
   ],
   "source": [
    "# choosing the best performing Random Forest Model\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "rf_model = rf.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = rf_model.predict(X_test)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 322.161 / Predict time: 0.23 ---- Precision: 0.971 / Recall: 0.823 / Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# choosing the best performing Random Forest Model\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n",
    "\n",
    "start = time.time()\n",
    "gb_model = gb.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = gb_model.predict(X_test)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional on Power transformations\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "data = pt.fit_transform(data)\n",
    "# convert the array back to a dataframe\n",
    "\n",
    "# SMOTE for imbalanced classification with python\n",
    "# How to Selectively Scale Numerical Input Variables for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
