{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Kaggle tutorial [NLP with spaCy](https://www.kaggle.com/learn/natural-language-processing)           \n",
    "DATACAMP :: Advanced_NLP_with_spaCy              \n",
    "Free version of Datacamp course [spaCy](https://course.spacy.io/en/)                 \n",
    "\n",
    "Tags: NLP, spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.util import minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting help\n",
    "\n",
    "# help(plt.hist)\n",
    "# help(plt.legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#print all rows of a df in ipython shell \n",
    "pd.set_option('display.max_rows', None)\n",
    "#print all columns of a df in ipython shell \n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath(os.getcwd())\n",
    "datadir = 'data'\n",
    "full_path = os.path.join(path, datadir)\n",
    "spam_file = os.path.join(full_path, \"datasets_483_982_spam.csv\")\n",
    "rawData = pd.read_csv(spam_file, encoding = \"ISO-8859-1\")\n",
    "fullCorpus = rawData[['v1', 'v2']]\n",
    "fullCorpus.head()\n",
    "fullCorpus.columns = ['label', 'text'];\n",
    "fullCorpus.head()\n",
    "spam = fullCorpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for case study #1\n",
    "\n",
    "# big data file\n",
    "yelp_file = os.path.join(full_path, \"yelp_academic_dataset_review.csv.zip\")\n",
    "rawData = pd.read_csv(yelp_file)\n",
    "\n",
    "reviews = rawData[:100]\n",
    "'''\n",
    "# Dataframes implement the Pandas API, use dask for large files\n",
    "import dask.dataframe as dd\n",
    "df = dd.read_csv('s3://.../2018-*-*.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'hG7b0MtEbXx5QzbzE6C_VA'</td>\n",
       "      <td>b'Total bill for this horrible service? Over $...</td>\n",
       "      <td>b'2013-05-07 04:34:36'</td>\n",
       "      <td>b'Q1sbwvVQXV2734tPgoKj4Q'</td>\n",
       "      <td>b'ujmEBvifdJM6h6RLv4wQIg'</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'yXQM5uF2jS6es16SJzNHfg'</td>\n",
       "      <td>b\"I *adore* Travis at the Hard Rock's new Kell...</td>\n",
       "      <td>b'2017-01-14 21:30:33'</td>\n",
       "      <td>b'GJXCdrto3ASJOqKeVWPi6Q'</td>\n",
       "      <td>b'NZnhc2sEQy3RmzKTZnqtwQ'</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'n6-Gk65cPZL6Uz8qRm3NYw'</td>\n",
       "      <td>b\"I have to say that this office really has it...</td>\n",
       "      <td>b'2016-11-09 20:09:03'</td>\n",
       "      <td>b'2TzJjDVDEuAW6MR5Vuc1ug'</td>\n",
       "      <td>b'WTqjgwHlXbSFevF32_DJVw'</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'dacAIZ6fTM6mqwW5uxkskg'</td>\n",
       "      <td>b\"Went in for a lunch. Steak sandwich was deli...</td>\n",
       "      <td>b'2018-01-09 20:56:38'</td>\n",
       "      <td>b'yi0R0Ugj_xUx_Nek0-_Qig'</td>\n",
       "      <td>b'ikCg8xy5JIg_NGPx-MSIDA'</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'ssoyf2_x0EQMed6fgHeMyQ'</td>\n",
       "      <td>b'Today was my second out of three sessions I ...</td>\n",
       "      <td>b'2018-01-30 23:07:38'</td>\n",
       "      <td>b'11a8sVPMUFtaC7_ABRkmtw'</td>\n",
       "      <td>b'b1b1eb3uo-w561D0ZfCEiQ'</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     user_id  \\\n",
       "0  b'hG7b0MtEbXx5QzbzE6C_VA'   \n",
       "1  b'yXQM5uF2jS6es16SJzNHfg'   \n",
       "2  b'n6-Gk65cPZL6Uz8qRm3NYw'   \n",
       "3  b'dacAIZ6fTM6mqwW5uxkskg'   \n",
       "4  b'ssoyf2_x0EQMed6fgHeMyQ'   \n",
       "\n",
       "                                                text                    date  \\\n",
       "0  b'Total bill for this horrible service? Over $...  b'2013-05-07 04:34:36'   \n",
       "1  b\"I *adore* Travis at the Hard Rock's new Kell...  b'2017-01-14 21:30:33'   \n",
       "2  b\"I have to say that this office really has it...  b'2016-11-09 20:09:03'   \n",
       "3  b\"Went in for a lunch. Steak sandwich was deli...  b'2018-01-09 20:56:38'   \n",
       "4  b'Today was my second out of three sessions I ...  b'2018-01-30 23:07:38'   \n",
       "\n",
       "                   review_id                business_id  funny  cool  useful  \\\n",
       "0  b'Q1sbwvVQXV2734tPgoKj4Q'  b'ujmEBvifdJM6h6RLv4wQIg'      1     0       6   \n",
       "1  b'GJXCdrto3ASJOqKeVWPi6Q'  b'NZnhc2sEQy3RmzKTZnqtwQ'      0     0       0   \n",
       "2  b'2TzJjDVDEuAW6MR5Vuc1ug'  b'WTqjgwHlXbSFevF32_DJVw'      0     0       3   \n",
       "3  b'yi0R0Ugj_xUx_Nek0-_Qig'  b'ikCg8xy5JIg_NGPx-MSIDA'      0     0       0   \n",
       "4  b'11a8sVPMUFtaC7_ABRkmtw'  b'b1b1eb3uo-w561D0ZfCEiQ'      0     0       7   \n",
       "\n",
       "   stars  \n",
       "0    1.0  \n",
       "1    5.0  \n",
       "2    5.0  \n",
       "3    5.0  \n",
       "4    1.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spam.head(10)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to NLP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Data comes in many different forms: time stamps, sensor readings, images, categorical labels, and so much more. But text is still some of the most valuable data out there for those who know how to use it.\n",
    "\n",
    "In this course about Natural Language Processing (NLP), you will use the leading NLP library (spaCy) to take on some of the most important tasks in working with text.\n",
    "\n",
    "By the end, you will be able to use spaCy for:\n",
    "\n",
    "    Basic text processing and pattern matching\n",
    "    Building machine learning models with text\n",
    "    Representing text with word embeddings that numerically capture the meaning of words and documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLP with spaCy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spaCy is the leading library for NLP, and it has quickly become one of the most popular Python frameworks. Most people find it intuitive, and it has excellent documentation.\n",
    "\n",
    "spaCy relies on models that are language-specific and come in different sizes. You can load a spaCy model with spacy.load.\n",
    "\n",
    "For example, here's how you would load the English language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en\n",
    "#!python -m spacy download en_core_web_lg\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load english model \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# with the model create a doc to be processed\n",
    "doc = nlp(\"Tea is healthy and calming, don't you think?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "This returns a document object that contains tokens. A token is a unit of text in the document, such as individual words and punctuation. SpaCy splits contractions like \"don't\" into two tokens, \"do\" and \"n't\". You can see the tokens by iterating through the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea\n",
      "is\n",
      "healthy\n",
      "and\n",
      "calming\n",
      ",\n",
      "do\n",
      "n't\n",
      "you\n",
      "think\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "# tokens\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "There are a few types of preprocessing to improve how we model with words. The first is \"lemmatizing.\" The \"lemma\" of a word is its base form. For example, \"walk\" is the lemma of the word \"walking\". So, when you lemmatize the word walking, you would convert it to walk.\n",
    "\n",
    "It's also common to remove stopwords. Stopwords are words that occur frequently in the language and don't contain much information. English stopwords include \"the\", \"is\", \"and\", \"but\", \"not\".\n",
    "\n",
    "With a spaCy token, token.lemma_ returns the lemma, while token.is_stop returns a boolean True if the token is a stopword (and False otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token \t\tLemma \t\tStopword\n",
      "----------------------------------------\n",
      "Tea\t\ttea\t\tFalse\n",
      "is\t\tbe\t\tTrue\n",
      "healthy\t\thealthy\t\tFalse\n",
      "and\t\tand\t\tTrue\n",
      "calming\t\tcalm\t\tFalse\n",
      ",\t\t,\t\tFalse\n",
      "do\t\tdo\t\tTrue\n",
      "n't\t\tnot\t\tTrue\n",
      "you\t\t-PRON-\t\tTrue\n",
      "think\t\tthink\t\tFalse\n",
      "?\t\t?\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "# print the lemma and a boolean if stopword \n",
    "print(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\n",
    "print(\"-\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Why are lemmas and identifying stopwords important? Language data has a lot of noise mixed in with informative content. In the sentence above, the important words are tea, healthy and calming. Removing stop words might help the predictive model focus on relevant words. Lemmatizing similarly helps by combining multiple forms of the same word into one base form (\"calming\", \"calms\", \"calmed\" would all change to \"calm\").\n",
    "\n",
    "However, lemmatizing and dropping stopwords might result in your models performing worse. So you should treat this preprocessing as part of your hyperparameter optimization process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Another common NLP task is matching tokens or phrases within chunks of text or whole documents. You can do pattern matching with regular expressions, but spaCy's matching capabilities tend to be easier to use.\n",
    "\n",
    "To match individual tokens, you create a Matcher. When you want to match a list of terms, it's easier and more efficient to use PhraseMatcher. For example, if you want to find where different smartphone models show up in some text, you can create patterns for the model names of interest. First you create the PhraseMatcher itself.\n",
    "\n",
    "\n",
    "The matcher is created using the vocabulary of your model. Here we're using the small English model you loaded earlier. Setting attr='LOWER' will match the phrases on lowercased text. This provides case insensitive matching.\n",
    "\n",
    "Next you create a list of terms to match in the text. The phrase matcher needs the patterns as document objects. The easiest way to get these is with a list comprehension using the nlp model.\n",
    "\n",
    "Then you create a document from the text to search and use the phrase matcher to find where the terms occur in the text.\n",
    "\n",
    "The matches here are a tuple of the match id and the positions of the start and end of the phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)]\n",
      "TerminologyList iPhone 11\n",
      "TerminologyList Galaxy Note\n",
      "TerminologyList iPhone XS\n",
      "TerminologyList Google Pixel\n"
     ]
    }
   ],
   "source": [
    "# pattern matching\n",
    "# create a matcher object\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "\n",
    "# create a list of terms to match in the text\n",
    "terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\n",
    "\n",
    "# matcher needs the patterns as document objects\n",
    "patterns = [nlp(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", patterns)\n",
    "\n",
    "# Borrowed from https://daringfireball.net/linked/2019/09/21/patel-11-pro\n",
    "text_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n",
    "               \"photography tests pitting the iPhone 11 Pro against the \"\n",
    "               \"Galaxy Note 10 Plus and last year’s iPhone XS and Google Pixel 3.\") \n",
    "\n",
    "# create doc from text to search for\n",
    "matches = matcher(text_doc)\n",
    "print(matches)\n",
    "\n",
    "match_id, start, end = matches[0]\n",
    "print(nlp.vocab.strings[match_id], text_doc[start:end])\n",
    "match_id, start, end = matches[1]\n",
    "print(nlp.vocab.strings[match_id], text_doc[start:end])\n",
    "match_id, start, end = matches[2]\n",
    "print(nlp.vocab.strings[match_id], text_doc[start:end])\n",
    "match_id, start, end = matches[3]\n",
    "print(nlp.vocab.strings[match_id], text_doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with SpaCy using bag of words\n",
    "\n",
    "A common task in NLP is text classification. This is \"classification\" in the conventional machine learning sense, and it is applied to text. Examples include spam detection, sentiment analysis, and tagging customer queries.\n",
    "\n",
    "In this tutorial, you'll learn text classification with spaCy. The classifier will detect spam messages, a common functionality in most email clients. Here is an overview of the data you'll use:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Machine learning models don't learn from raw text data. Instead, you need to convert the text to something numeric.\n",
    "\n",
    "The simplest common representation is a variation of one-hot encoding. You represent each document as a vector of term frequencies for each term in the vocabulary. The vocabulary is built from all the tokens (terms) in the corpus (the collection of documents).\n",
    "\n",
    "As an example, take the sentences \"Tea is life. Tea is love.\" and \"Tea is healthy, calming, and delicious.\" as our corpus. The vocabulary then is {\"tea\", \"is\", \"life\", \"love\", \"healthy\", \"calming\", \"and\", \"delicious\"} (ignoring punctuation).\n",
    "\n",
    "For each document, count up how many times a term occurs, and place that count in the appropriate element of a vector. The first sentence has \"tea\" twice and that is the first position in our vocabulary, so we put the number 2 in the first element of the vector. Our sentences as vectors then look like\n",
    "\n",
    "v1=[22110000]\n",
    "v2=[11001111]\n",
    "\n",
    "This is called the bag of words representation. You can see that documents with similar terms will have similar vectors. Vocabularies frequently have tens of thousands of terms, so these vectors can be very large.\n",
    "\n",
    "Another common representation is TF-IDF (Term Frequency - Inverse Document Frequency). TF-IDF is similar to bag of words except that each term count is scaled by the term's frequency in the corpus. Using TF-IDF can potentially improve your models. You won't need it here. Feel free to look it up though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Bag of Words model\n",
    "\n",
    "Once you have your documents in a bag of words representation, you can use those vectors as input to any machine learning model. spaCy handles the bag of words conversion and building a simple linear model for you with the TextCategorizer class.\n",
    "\n",
    "The TextCategorizer is a spaCy pipe. Pipes are classes for processing and transforming tokens. When you create a spaCy model with nlp = spacy.load('en_core_web_sm'), there are default pipes that perform part of speech tagging, entity recognition, and other transformations. When you run text through a model doc = nlp(\"Some text here\"), the output of the pipes are attached to the tokens in the doc object. The lemmas for token.lemma_ come from one of these pipes.\n",
    "\n",
    "You can remove or add pipes to models. What we'll do here is create an empty model without any pipes (other than a tokenizer, since all models always have a tokenizer). Then, we'll create a TextCategorizer pipe and add it to the empty model.\n",
    "\n",
    "Since the classes are either ham or spam, we set \"exclusive_classes\" to True. We've also configured it with the bag of words (\"bow\") architecture. spaCy provides a convolutional neural network architecture as well, but it's more complex than you need for now.\n",
    "\n",
    "Next we'll add the labels to the model. Here \"ham\" are for the real messages, \"spam\" are spam messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a TextCategorizer model\n",
    "# Create an empty model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" architecture\n",
    "textcat = nlp.create_pipe(\n",
    "              \"textcat\",\n",
    "              config={\n",
    "                \"exclusive_classes\": True,\n",
    "                \"architecture\": \"bow\"})\n",
    "\n",
    "# Add the TextCategorizer to the empty model\n",
    "nlp.add_pipe(textcat)\n",
    "\n",
    "# Add labels to text classifier\n",
    "textcat.add_label(\"ham\")\n",
    "textcat.add_label(\"spam\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Text Categorizer Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, you'll convert the labels in the data to the form TextCategorizer requires. For each document, you'll create a dictionary of boolean values for each class.\n",
    "\n",
    "For example, if a text is \"ham\", we need a dictionary {'ham': True, 'spam': False}. The model is looking for these labels inside another dictionary with the key 'cats'.\n",
    "\n",
    "Then we combine the texts and labels into a single list.\n",
    "\n",
    "Now you are ready to train the model. First, create an optimizer using nlp.begin_training(). spaCy uses this optimizer to update the model. In general it's more efficient to train models in small batches. spaCy provides the minibatch function that returns a generator yielding minibatches for training. Finally, the minibatches are split into texts and labels, then used with nlp.update to update the model's parameters.\n",
    "\n",
    "This is just one training loop (or epoch) through the data. The model will typically need multiple epochs. Use another loop for more epochs, and optionally re-shuffle the training data at the begining of each loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "  {'cats': {'ham': True, 'spam': False}}),\n",
       " ('Ok lar... Joking wif u oni...', {'cats': {'ham': True, 'spam': False}}),\n",
       " (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       "  {'cats': {'ham': False, 'spam': True}})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training data\n",
    "\n",
    "train_texts = spam['text'].values\n",
    "train_labels = [{'cats': {'ham': label == 'ham',\n",
    "                          'spam': label == 'spam'}} \n",
    "                for label in spam['label']]\n",
    "train_data = list(zip(train_texts, train_labels))\n",
    "train_data[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'textcat': 1.3595883706348104}\n",
      "{'textcat': 1.7019103596425111}\n",
      "{'textcat': 1.8889567541276335}\n",
      "{'textcat': 2.0086188285025415}\n",
      "{'textcat': 2.0908757135393223}\n",
      "{'textcat': 2.142813575270255}\n",
      "{'textcat': 2.1792574747399316}\n",
      "{'textcat': 2.205051286399283}\n",
      "{'textcat': 2.224066504236819}\n",
      "{'textcat': 2.238125779956701}\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "\n",
    "random.seed(1)\n",
    "spacy.util.fix_random_seed(1)\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "losses = {}\n",
    "for epoch in range(10):\n",
    "    random.shuffle(train_data)\n",
    "    # Create the batch generator with batch size = 8\n",
    "    batches = minibatch(train_data, size=8)\n",
    "    # Iterate through minibatches\n",
    "    for batch in batches:\n",
    "        # Each batch is a list of (text, label) but we need to\n",
    "        # send separate lists for texts and labels to update().\n",
    "        # This is a quick way to split a list of tuples into lists\n",
    "        texts, labels = zip(*batch)\n",
    "        nlp.update(texts, labels, sgd=optimizer, losses=losses)\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that you have a trained model, you can make predictions with the predict() method. The input text needs to be tokenized with nlp.tokenizer. Then you pass the tokens to the predict method which returns scores. The scores are the probability the input text belongs to the classes.\n",
    "\n",
    "The scores are used to predict a single class or label by choosing the label with the highest probability. You get the index of the highest probability with scores.argmax, then use the index to get the label string from textcat.labels.\n",
    "\n",
    "Evaluating the model is straightforward once you have the predictions. To measure the accuracy, calculate how many correct predictions are made on some test data, divided by the total number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9995530e-01 4.4716071e-05]\n",
      " [1.3936103e-02 9.8606384e-01]]\n",
      "['ham', 'spam']\n"
     ]
    }
   ],
   "source": [
    "# making predictions\n",
    "\n",
    "texts = [\"Are you ready for the tea party????? It's gonna be wild\",\n",
    "         \"URGENT Reply to this message for GUARANTEED FREE TEA\" ]\n",
    "docs = [nlp.tokenizer(text) for text in texts]\n",
    "    \n",
    "# Use textcat to get the scores for each doc\n",
    "textcat = nlp.get_pipe('textcat')\n",
    "scores, _ = textcat.predict(docs)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "# From the scores, find the label with the highest score/probability\n",
    "predicted_labels = scores.argmax(axis=1)\n",
    "print([textcat.labels[label] for label in predicted_labels])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with SpaCy using Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You know at this point that machine learning on text requires that you first represent the text numerically. So far, you've done this with bag of words representations. But you can usually do better with word embeddings.\n",
    "\n",
    "Word embeddings (also called word vectors) represent each word numerically in such a way that the vector corresponds to how that word is used or what it means. Vector encodings are learned by considering the context in which the words appear. Words that appear in similar contexts will have similar vectors. For example, vectors for \"leopard\", \"lion\", and \"tiger\" will be close together, while they'll be far away from \"planet\" and \"castle\".\n",
    "\n",
    "Even cooler, relations between words can be examined with mathematical operations. Subtracting the vectors for \"man\" and \"woman\" will return another vector. If you add that to the vector for \"king\" the result is close to the vector for \"queen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](img/WordEmbeddings.PNG)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "These vectors can be used as features for machine learning models. Word vectors will typically improve the performance of your models above bag of words encoding. spaCy provides embeddings learned from a model called Word2Vec. You can access them by loading a large language model like en_core_web_lg. Then they will be available on tokens from the .vector attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to load the large model to get the vectors\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disabling other pipes because we don't need them and it'll speed up this part a bit\n",
    "text = \"These vectors can be used as features for machine learning models.\"\n",
    "with nlp.disable_pipes():\n",
    "    vectors = np.array([token.vector for token in  nlp(text)])\n",
    "\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "These are 300-dimensional vectors, with one vector for each word. However, we only have document-level labels and our models won't be able to use the word-level embeddings. So, you need a vector representation for the entire document.\n",
    "\n",
    "There are many ways to combine all the word vectors into a single document vector we can use for model training. A simple and surprisingly effective approach is simply averaging the vectors for each word in the document. Then, you can use these document vectors for modeling.\n",
    "\n",
    "spaCy calculates the average document vector which you can get with doc.vector. Here is an example loading the spam data and converting it to document vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 300)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ham is the label for non-spam messages\n",
    "\n",
    "with nlp.disable_pipes():\n",
    "    doc_vectors = np.array([nlp(text).vector for text in spam.text])\n",
    "    \n",
    "doc_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models\n",
    "\n",
    "With the document vectors, you can train scikit-learn models, xgboost models, or any other standard approach to modeling. \n",
    "\n",
    "Here is an example using support vector machines (SVMs). Scikit-learn provides an SVM classifier LinearSVC. This works similar to other scikit-learn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_vectors, spam.label,\n",
    "                                                    test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.312%\n"
     ]
    }
   ],
   "source": [
    "# Set dual=False to speed up training, and it's not needed\n",
    "svc = LinearSVC(random_state=1, dual=False, max_iter=10000)\n",
    "svc.fit(X_train, y_train)\n",
    "print(f\"Accuracy: {svc.score(X_test, y_test) * 100:.3f}%\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents similarity \n",
    "\n",
    "Documents with similar content generally have similar vectors. So you can find similar documents by measuring the similarity between the vectors. A common metric for this is the cosine similarity which measures the angle between two vectors, a and b\n",
    "\n",
    "cosθ=a⋅b/∥a∥∥b∥\n",
    "\n",
    "This is the dot product of a and b, divided by the magnitudes of each vector. The cosine similarity can vary between -1 and 1, corresponding complete opposite to perfect similarity, respectively. To calculate it, you can use the metric from scikit-learn or write your own function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    return a.dot(b)/np.sqrt(a.dot(a) * b.dot(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7030031"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nlp(\"REPLY NOW FOR FREE TEA\").vector\n",
    "b = nlp(\"According to legend, Emperor Shen Nung discovered tea when leaves from a wild tree blew into his pot of boiling water.\").vector\n",
    "cosine_similarity(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study #1 :: Sentiment analysis\n",
    "\n",
    "Vectorizing Language\n",
    "\n",
    "Embeddings are both conceptually clever and practically effective.\n",
    "\n",
    "So let's try them for the sentiment analysis model you built for the restaurant. Then you can find the most similar review in the data set given some example text. It's a task where you can easily judge for yourself how well the embeddings work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 300)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We just want the vectors so we can turn off other models in the pipeline\n",
    "with nlp.disable_pipes():\n",
    "    vectors = np.array([nlp(review.text).vector for idx, review in reviews.iterrows()])\n",
    "    \n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model on Document Vectors\n",
    "\n",
    "Next you'll train a LinearSVC model using the document vectors. It runs pretty quick and works well in high dimensional settings like you have here.\n",
    "\n",
    "After running the LinearSVC model, you might try experimenting with other types of models to see whether it improves your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'hG7b0MtEbXx5QzbzE6C_VA'</td>\n",
       "      <td>b'Total bill for this horrible service? Over $...</td>\n",
       "      <td>b'2013-05-07 04:34:36'</td>\n",
       "      <td>b'Q1sbwvVQXV2734tPgoKj4Q'</td>\n",
       "      <td>b'ujmEBvifdJM6h6RLv4wQIg'</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'yXQM5uF2jS6es16SJzNHfg'</td>\n",
       "      <td>b\"I *adore* Travis at the Hard Rock's new Kell...</td>\n",
       "      <td>b'2017-01-14 21:30:33'</td>\n",
       "      <td>b'GJXCdrto3ASJOqKeVWPi6Q'</td>\n",
       "      <td>b'NZnhc2sEQy3RmzKTZnqtwQ'</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'n6-Gk65cPZL6Uz8qRm3NYw'</td>\n",
       "      <td>b\"I have to say that this office really has it...</td>\n",
       "      <td>b'2016-11-09 20:09:03'</td>\n",
       "      <td>b'2TzJjDVDEuAW6MR5Vuc1ug'</td>\n",
       "      <td>b'WTqjgwHlXbSFevF32_DJVw'</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'dacAIZ6fTM6mqwW5uxkskg'</td>\n",
       "      <td>b\"Went in for a lunch. Steak sandwich was deli...</td>\n",
       "      <td>b'2018-01-09 20:56:38'</td>\n",
       "      <td>b'yi0R0Ugj_xUx_Nek0-_Qig'</td>\n",
       "      <td>b'ikCg8xy5JIg_NGPx-MSIDA'</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'ssoyf2_x0EQMed6fgHeMyQ'</td>\n",
       "      <td>b'Today was my second out of three sessions I ...</td>\n",
       "      <td>b'2018-01-30 23:07:38'</td>\n",
       "      <td>b'11a8sVPMUFtaC7_ABRkmtw'</td>\n",
       "      <td>b'b1b1eb3uo-w561D0ZfCEiQ'</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     user_id  \\\n",
       "0  b'hG7b0MtEbXx5QzbzE6C_VA'   \n",
       "1  b'yXQM5uF2jS6es16SJzNHfg'   \n",
       "2  b'n6-Gk65cPZL6Uz8qRm3NYw'   \n",
       "3  b'dacAIZ6fTM6mqwW5uxkskg'   \n",
       "4  b'ssoyf2_x0EQMed6fgHeMyQ'   \n",
       "\n",
       "                                                text                    date  \\\n",
       "0  b'Total bill for this horrible service? Over $...  b'2013-05-07 04:34:36'   \n",
       "1  b\"I *adore* Travis at the Hard Rock's new Kell...  b'2017-01-14 21:30:33'   \n",
       "2  b\"I have to say that this office really has it...  b'2016-11-09 20:09:03'   \n",
       "3  b\"Went in for a lunch. Steak sandwich was deli...  b'2018-01-09 20:56:38'   \n",
       "4  b'Today was my second out of three sessions I ...  b'2018-01-30 23:07:38'   \n",
       "\n",
       "                   review_id                business_id  funny  cool  useful  \\\n",
       "0  b'Q1sbwvVQXV2734tPgoKj4Q'  b'ujmEBvifdJM6h6RLv4wQIg'      1     0       6   \n",
       "1  b'GJXCdrto3ASJOqKeVWPi6Q'  b'NZnhc2sEQy3RmzKTZnqtwQ'      0     0       0   \n",
       "2  b'2TzJjDVDEuAW6MR5Vuc1ug'  b'WTqjgwHlXbSFevF32_DJVw'      0     0       3   \n",
       "3  b'yi0R0Ugj_xUx_Nek0-_Qig'  b'ikCg8xy5JIg_NGPx-MSIDA'      0     0       0   \n",
       "4  b'11a8sVPMUFtaC7_ABRkmtw'  b'b1b1eb3uo-w561D0ZfCEiQ'      0     0       7   \n",
       "\n",
       "   stars  \n",
       "0    1.0  \n",
       "1    5.0  \n",
       "2    5.0  \n",
       "3    5.0  \n",
       "4    1.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19047938,  0.1821789 , -0.02618041, ..., -0.04719066,\n",
       "         0.01550798,  0.09590381],\n",
       "       [-0.0256067 ,  0.1500729 , -0.11254921, ..., -0.04775102,\n",
       "         0.03123375,  0.06923966],\n",
       "       [-0.07813025,  0.1925469 , -0.14057271, ..., -0.04411409,\n",
       "         0.06611802,  0.08484872],\n",
       "       ...,\n",
       "       [-0.03007777,  0.16273198, -0.08914592, ..., -0.15289608,\n",
       "         0.1121713 ,  0.07087018],\n",
       "       [-0.04080139,  0.22955525, -0.13488048, ..., -0.05866174,\n",
       "         0.01276404,  0.0743405 ],\n",
       "       [-0.02303501,  0.15512745, -0.08919447, ..., -0.04124865,\n",
       "         0.03225725,  0.04699771]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-abd7cc5d26a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train, test split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m X_train, X_test, y_train, y_test = train_test_split(vectors, reviews.sentiment, \n\u001b[0m\u001b[0;32m      3\u001b[0m                                                     test_size=0.1, random_state=1)\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Create the LinearSVC model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Applications\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5272\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5273\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5274\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'sentiment'"
     ]
    }
   ],
   "source": [
    "# ERROR, sentiment(target) is missing\n",
    "# train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, reviews.sentiment, \n",
    "                                                    test_size=0.1, random_state=1)\n",
    "\n",
    "# Create the LinearSVC model\n",
    "model = LinearSVC(random_state=1, dual=False)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Uncomment and run to see model accuracy\n",
    "print(f'Model test accuracy: {model.score(X_test, y_test)*100:.3f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
